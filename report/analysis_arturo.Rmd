## **Supervised Learning**

```{r, echo = FALSE, message = FALSE, warning=FALSE}
source(here::here("scripts/setup.R"))

aisles <- read.csv(here::here("data/aisles.csv"), header = TRUE)
departments <- read.csv(here::here("data/departments.csv"), header = TRUE)
products <- read.csv(here::here("data/products.csv"), header = TRUE)
order_products_train <- read.csv(here::here("data/order_products__train.csv"), header = TRUE)
orders <- read.csv(here::here("data/orders.csv"), header = TRUE)

user_purchases <- orders %>%
  filter(eval_set == "train") %>%
  left_join(order_products_train, by = "order_id") %>%
  left_join(products, by = "product_id") %>%
  left_join(aisles, by = c("aisle_id")) %>%
  left_join(departments, by = c("department_id")) %>%
  select(order_id, order_dow, order_hour_of_day, product_id, aisle_id, aisle,
         department)


```

### **Data Preparation for Models**

Before starting applying the models to the data, we have decided to aggregate the column called `id_orders` by department, so we could know the number of products purchased by department. In addition, we have considered to keep the column `order_dow`, to identify on which day of the week an order was purchased.

After creating this new table, we converted the column `order_dow` from numeric(int) to categorical values(factor), and to understand better this values, we change the integer values to the name of the day of the week. For example: The value `0` was transformed to `Sunday`, `1` to `Monday`, `2` to `Tuesday`, and so on. 

```{r echo=FALSE, message=FALSE}
#Preparation for the analysis
orders <- orders %>%
  filter(eval_set == "train") %>%
  select(order_id, order_dow, order_hour_of_day)

department_wide <- user_purchases %>%
  group_by(order_id, department) %>%
  summarise(product_num = n()) %>%
  pivot_wider(names_from = "department", values_from = "product_num") %>%
  left_join(orders, by = "order_id") %>%
  relocate(order_dow, .before = `canned goods`) %>%
  relocate(order_hour_of_day, .before = `canned goods`)

department_wide[is.na(department_wide)] <- 0

# Selecting only the meaningful columns for the models
dep_data <- department_wide
data_by_day <- dep_data %>%
  as.data.frame() %>%
  select(-order_id, -order_hour_of_day)
data_by_day$order_dow[data_by_day$order_dow == 0] <- "Sunday"
data_by_day$order_dow[data_by_day$order_dow == 1] <- "Monday"
data_by_day$order_dow[data_by_day$order_dow == 2] <- "Tuesday"
data_by_day$order_dow[data_by_day$order_dow == 3] <- "Wednesday"
data_by_day$order_dow[data_by_day$order_dow == 4] <- "Thursday"
data_by_day$order_dow[data_by_day$order_dow == 5] <- "Friday"
data_by_day$order_dow[data_by_day$order_dow == 6] <- "Saturday"

# Transform y variable to a factor
data_by_day$order_dow  <- as.factor(data_by_day$order_dow)

# Make Valid Column Names 
colnames(data_by_day) <- make.names(colnames(data_by_day))

```

### **Decision Trees - Classification**

Decision trees are algorithms that recursively search the space for the best boundary possible, until we unable them to do so (Ivo Bernardo,2021). The basic functionality of decision trees is to split the data space into rectangles, by measuring each split. The main goal is to minimize the impurity of each split from the previous one. 

Our goal is to determine which day of the week a given order will be placed. Due to the fact that we have transformed the column `order_dow` as a factor with categorical values, we will apply decision trees with classification task. In addition, we will apply this type of model on 3 different approaches:

* One day of the week - Unbalanced data
* One day of the week - Balanced data with Sub-sampling and Cross-Validation
* Weekdays and Weekend - Balanced data with Sub-sampling and Cross-Validation

> One day of the week - Unbalanced data

For this approach we want to measure the accuracy of the model with the unbalanced data. In addition, it will be interesting to see which departments were considered the best to split the data into days of the week to later be compared to a balanced data with cross-validation (second approach). 


```{r echo=FALSE, message=FALSE}

# Separate our data into Training set and Test set
set.seed(12345) # for reproducibility
index.tr <- createDataPartition(y = data_by_day$order_dow, p= 0.8,list = FALSE)
df.tr <- data_by_day[index.tr,]
df.te <- data_by_day[-index.tr,]

# Classification tree fit and plot
set.seed(12345)
m1_tree_byday <- rpart(order_dow ~ ., method= "class", 
                     data=df.tr, cp= 0.0001, model=TRUE)

#Find the best cp
#plotcp(m1_tree_byday)

# Prune the tree based on the best cp value
m1_tree_byday_pruned <- prune(m1_tree_byday, cp=0.00045)

# Plot the pruned tree
rpart.plot(m1_tree_byday_pruned, type = 5, digits = 3, fallen.leaves = F,
           tweak=1.5 )
#rpart.rules(m1_tree_byday_pruned, cover = T)
```

According to the pruned tree, we observe that the department **produce** have the most relevance within the departments, this could be influenced by the fact that this department has the highest number of products purchased in our data set. Furthermore, the tree show us that with an amount of products purchased higher or equal than 3, the model will classify the day of the week as Sunday (*44.9% of the data fall into this category*), if it is lower than 3 the tree will split into another node containing the **frozen** department.

Likewise, the same procedure will be consider for this node and the following, they will start from the previous node and will try to minimize the impurity at each split. It should be noted that we cannot observed on the terminal nodes all the days of the week, because of the way in which the trees are generated. For the same reason, we expect on the prediction of the test set, a prediction value of "0" on the days of the week different to Sunday and Monday.

```{r echo=FALSE, message=FALSE}
# Predicting
m1_tree_byday_pruned_pred <- predict(m1_tree_byday_pruned, newdata = df.te, type="class")


# Measure the accuracy of the prediction
confusionMatrix(data=as.factor(m1_tree_byday_pruned_pred), reference = df.te$order_dow)
```

As expected, only on Sunday and Monday we get predictive values for all the days of the week, while in the rest we get cero. Overall, the accuracy of this model is low with a score of **0.22**, meaning that the model have `(0.22 - (1/7))= 0.077` around **8%** of accuracy classifying the days of the week. It is important to recall that there is a big difference between sensitivity and specificity because our data is not balanced.

> One day of the week - Balanced data with Sub-sampling and Cross-Validation

Now for this approach, we will balanced the data with sub-sampling and make the overall score more robust by applying to the model a cross-validation technique, this will help us to find the best set of hyperparameters.

```{r echo=FALSE, message=FALSE}

# Building a Classification tree model: Considering a Repeated Cross-Validation with a Class balancing of Sub-sampling
set.seed(12345) 
m2_tree_byday <- caret::train(order_dow ~ .,
                           data = df.tr,
                           method ="rpart",
                           preProcess = NULL,
                           trControl=trainControl(method="repeatedcv", number=10,
                                                  repeats=10, verboseIter=FALSE,
                                                  sampling="down"))

# Plot the Tree
rpart.plot(m2_tree_byday$finalModel, type = 5, digits = 3, fallen.leaves = F,
           tweak=1.5)
rpart.rules(m2_tree_byday$finalModel, cover = T)
```

The left column (*.outcome*) of the rules show the day that was selected for the terminal node (*the one with the highest probability*) and next to it the probability of each day of the week for the department selected. In this case for the last rule it seems that Wednesday and Thursday have the same probability because of the rounding, but Thursday its **0.3%** above Wednesday, this can be seen from the tree plot.

The rightmost column (*cover*) gives the percentage of observations in each rule. The first rule says that Saturday will be chosen when the department produce is lower than 3 and higher or equal than 1 with a probability of **18.2%**. 

```{r echo=FALSE, message=FALSE}
# Apply Model to the test dataset
set.seed(12345)
m2_tree_byday_pred <- predict(m2_tree_byday, newdata=df.te)

# Measure the accuracy of the prediction
confusionMatrix(data=as.factor(m2_tree_byday_pred), reference = df.te$order_dow)
```

From the confusion matrix we observe a better result between the sensitivity and specificity across the classes, if we compare the previous model with this one, we notice that on the class Sunday the values of the sensitivity changed from **0.882** to **0.545**, and for the specificity from **0.194** to **0.576**. As expected the Accuracy has decreased from **0.211** to **0.195**, meaning that the model have `(0.195 - (1/7))= 0.052` around **5%** of accuracy per day of the week, but the Balanced Accuracy is better. This model would be preferred than the one used with unbalanced data.

> Weekdays and Weekend - Balanced data with Sub-sampling and Cross-Validation

For the Final approach we transformed the levels of the column `order_dow` into two, one for the days during the week and the other for the weekend. On top of that we balanced our levels "weekday" and "weekend" and consider a Cross-Validation to train the model.

```{r echo=FALSE, message=FALSE}
# Prepare Data
dep_data <- department_wide
data_by_day <- dep_data %>%
  as.data.frame() %>%
  select(-order_id, -order_hour_of_day) 
data_by_day$order_dow[data_by_day$order_dow == 0] <- "Sunday"
data_by_day$order_dow[data_by_day$order_dow == 1] <- "Monday"
data_by_day$order_dow[data_by_day$order_dow == 2] <- "Tuesday"
data_by_day$order_dow[data_by_day$order_dow == 3] <- "Wednesday"
data_by_day$order_dow[data_by_day$order_dow == 4] <- "Thursday"
data_by_day$order_dow[data_by_day$order_dow == 5] <- "Friday"
data_by_day$order_dow[data_by_day$order_dow == 6] <- "Saturday"

# Grouping days by Week and Weekend
data_group <- data_by_day %>% mutate(Group =ifelse(
  order_dow == "Sunday"|order_dow == "Saturday", "weekend", "weekday")) %>%
  select(-order_dow)

# Transform y variable to a factor
data_group$Group<- as.factor(data_group$Group)

# Make Valid Column Names 
colnames(data_group) <- make.names(colnames(data_group))

# Separate our data into Training set and Test set
set.seed(12345) # for reproducibility
index.tr <- createDataPartition(y = data_group$Group, p= 0.8,list = FALSE)
df.tr <- data_group[index.tr,]
df.te <- data_group[-index.tr,]

# Building a Classification tree model: Considering a Repeated Cross-Validation with a Class balancing of sub-sampling
set.seed(12345)
m3_tree_group <- caret::train(Group ~ .,
                           data = df.tr,
                           method ="rpart",
                           preProcess = NULL,
                           trControl=trainControl(method="cv",
                                                  number=10,
                                                  verboseIter=FALSE,
                                                  sampling="down"))

# Plot the Tree
rpart.plot(m3_tree_group$finalModel, type = 3, digits = 2, fallen.leaves = FALSE, cex = 0.5)
```


```{r echo=FALSE, message=FALSE}
# Apply Model to the test dataset
set.seed(12345)
m3_tree_group_pred <- predict(m3_tree_group, newdata=df.te)
table(Pred= m3_tree_group_pred, Obs=df.te$Group)

# Measure the accuracy of the prediction
confusionMatrix(data=as.factor(m3_tree_group_pred), reference = df.te$Group)

library(caret)
library(cvms)
library(patchwork)
table_3 <- as_tibble(table(tibble(Actual = df.te$Group,
                  Prediction = m3_tree_group_pred)))

plot_confusion_matrix(table_3, 
                      target_col = "Actual", 
                      prediction_col = "Prediction",
                      counts_col = "n",
                      palette = "Oranges") +
  ggplot2::labs(x = "Actual", y = "Prediction")

draw_confusion_matrix <- function(cm) {
  
  # add in the specifics 
  plot(c(70, 0), c(70, 0), type = "n", xlab="", ylab="", xaxt='n', yaxt='n', font=15)
  
  # add in the accuracy information 
  text(35, 60, names(cm$overall[1]), cex=1.3, font=15)
  text(35, 50, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(35, 40, names(cm$overall[2]), cex=1.3, font=15)
  text(35, 30, round(as.numeric(cm$overall[2]), 3), cex=1.4)
  text(35, 20, names(cm$byClass[11]), cex=1.3, font=15)
  text(35, 10, round(as.numeric(cm$byClass[11]), 3), cex=1.4)

}
cm<-confusionMatrix(m3_tree_group_pred, df.te$Group)
draw_confusion_matrix(cm)

```
Confusion Matrix and Statistics

          Reference
Prediction weekday weekend
   weekday    8740    3905
   weekend    8228    5368
                                        
               Accuracy : 0.538         
                 95% CI : (0.532, 0.544)
    No Information Rate : 0.647         
    P-Value [Acc > NIR] : 1             
                                        
                  Kappa : 0.085         
                                        
 Mcnemar's Test P-Value : <2e-16        
                                        
            Sensitivity : 0.515         
            Specificity : 0.579         
         Pos Pred Value : 0.691         
         Neg Pred Value : 0.395         
             Prevalence : 0.647         
         Detection Rate : 0.333         
   Detection Prevalence : 0.482         
      Balanced Accuracy : 0.547         
                                        
       'Positive' Class : weekday 
       
       
### **Random Forest**

> Weekdays and Weekend - Balanced data with Sub-sampling and Cross-Validation

```{r echo=FALSE, message=FALSE}

# sample from distinct values of user_id
data_group1 <- data_group %>%
  slice_head(n= 10000)

# Separate our data into Training set and Test set
set.seed(12345) # for reproducibility
index.tr <- createDataPartition(y = data_group1$Group, p= 0.8,list = FALSE)
df.tr <- data_group1[index.tr,]
df.te <- data_group1[-index.tr,]

# Build the model
set.seed(12345)
m4_rf_group <- caret::train(Group ~ .,
                         data=df.tr,
                         method="rf",
                         preProcess=NULL, 
                         trControl=trainControl(method="cv", 
                                                number=10,
                                                verboseIter=FALSE,
                                                sampling = "down"))
# It takes more than 3 min to run

varImp(m4_rf_group)

# Apply Model to the test dataset
set.seed(12345)
m4_rf_group_pred <- predict(m4_rf_group, newdata=df.te)

# Measure the accuracy of the prediction
confusionMatrix(data=as.factor(m4_rf_group_pred), reference = df.te$Group)

```


# Computed with the whole dataset (but it takes 1 hr to run, results are not a big diff)
Confusion Matrix and Statistics

          Reference
Prediction weekday weekend
   weekday    9583    4338
   weekend    7385    4935
                                        
               Accuracy : 0.553         
                 95% CI : (0.547, 0.559)
    No Information Rate : 0.647         
    P-Value [Acc > NIR] : 1             
                                        
                  Kappa : 0.09          
                                        
 Mcnemar's Test P-Value : <2e-16        
                                        
            Sensitivity : 0.565         
            Specificity : 0.532         
         Pos Pred Value : 0.688         
         Neg Pred Value : 0.401         
             Prevalence : 0.647         
         Detection Rate : 0.365         
   Detection Prevalence : 0.531         
      Balanced Accuracy : 0.548         
                                        
       'Positive' Class : weekday 
       
### Here the results are diferent but I don't think is that relevant
  only 20 most important variables shown (out of 21)

                Overall
produce          100.00
dairy.eggs        69.10
snacks            59.47
frozen            55.58
beverages         54.57
pantry            46.84
canned.goods      37.59
dry.goods.pasta   34.07
bakery            33.49
deli              33.24
household         32.87
breakfast         29.04
meat.seafood      27.77
personal.care     24.42
babies            19.31
international     16.21
missing           12.07
pets               7.41
alcohol            5.65
other              1.69

## References

Ivo Bernardo, Classification Decision Trees, Easily Explained, Aug 30, 2021: https://towardsdatascience.com/classification-decision-trees-easily-explained-f1064dde175e