---
title: "analysis-ting"
output: html_document
---

## supervised learning methods

### Model: Multinomial logistic regression

Our goal is to predict the day of the week that the order will be placed according to the product composition in the order. Since there are 7 days in a week so it is not a binary logistic regression problem but a multinomial logistic regression.

#### 1.train and test set
```{r}
department_wide$order_dow <- as.factor(department_wide$order_dow)
department_wide$order_hour_of_day <- as.factor(department_wide$order_hour_of_day)
set.seed(12345)
insta.index <- sample(x=1:nrow(department_wide), size=0.8*nrow(department_wide), replace=FALSE)
insta.tr <- department_wide[insta.index,]
insta.te <- department_wide[-insta.index,]
```


#### 2.build the model

We use the day 0 as the reference level. To build the model, we use the number of products in each deaprtment of the order as explantory variables.
```{r}
library(nnet)
insta.tr$order_dow2 <- relevel(as.factor(insta.tr$order_dow),ref = "0")
train <- insta.tr[4:25]
insta_multiglm <- multinom(order_dow2~.,data=train) 
```

#### 3.check the score of the model

```{r}
test <- insta.te[3:24]
insta_pred <- predict(insta_multiglm,newdata = test)
confusionMatrix(factor(insta_pred),factor(insta.te$order_dow))
```

#### improve the model

According to the confusion matrix, the accurancy is low and there is a big difference between sensitivity and specificity. We try to balance the data and use a cross-validation to improve the model accurancy.

##### 1.balance the data

Check the unbalance of classes frequencies.
```{r}
table(insta.tr$order_dow2)
```

##### 2.subsampling and cross-validation

We try the cross-validation with the subsampling data, but the data set is big and it takes a very long time to run it so we decide not to include the cross-validation.
```{r}
#insta_multiglm_new <- caret::train(order_dow2~.,data=train,
#                                   method="multinom",
#                                   trControl=trainControl(method = "cv",
#                                                          number = 10,
#                                                         sampling = "down"))
```


subsampling the data without cross-validation
```{r}
n.3 <- min(table(insta.tr$order_dow2)) ## 12627

insta.tr.0 <- filter(insta.tr, order_dow2=="0") ## the "3" cases
insta.tr.1 <- filter(insta.tr, order_dow2=="1")
insta.tr.2 <- filter(insta.tr, order_dow2=="2")
insta.tr.3 <- filter(insta.tr, order_dow2=="3")
insta.tr.4 <- filter(insta.tr, order_dow2=="4")
insta.tr.5 <- filter(insta.tr, order_dow2=="5")
insta.tr.6 <- filter(insta.tr, order_dow2=="6")

index.0 <- sample(size=n.3, x=1:nrow(insta.tr.0), replace=FALSE) 
index.1 <- sample(size=n.3, x=1:nrow(insta.tr.1), replace=FALSE)
index.2 <- sample(size=n.3, x=1:nrow(insta.tr.2), replace=FALSE)
index.4 <- sample(size=n.3, x=1:nrow(insta.tr.4), replace=FALSE)
index.5 <- sample(size=n.3, x=1:nrow(insta.tr.5), replace=FALSE)
index.6 <- sample(size=n.3, x=1:nrow(insta.tr.6), replace=FALSE)

insta.tr.subs <- data.frame(rbind(insta.tr.3, insta.tr.0[index.0,],
                               insta.tr.1[index.1,],insta.tr.2[index.2,],
                               insta.tr.4[index.4,],insta.tr.5[index.5,],
                               insta.tr.6[index.6,])) 
insta.tr.subs <- insta.tr.subs %>% rename("canned goods"="canned.goods",
                                    "dairy eggs"="dairy.eggs",
                      "meat seafood"="meat.seafood","personal care"="personal.care",
                      "dry goods pasta"="dry.goods.pasta"
                      )

table(insta.tr.subs$order_dow2)
```

```{r}
train_sub <- insta.tr.subs[3:25]
insta_multiglm_bal <- multinom(order_dow2~.,data=train_sub)
```

From the confusion matrix report we can notice that the balanced accurancy of each class is higher and the kappa of the model is also high.
```{r}
test_sub <- insta.te[3:24]
insta_pred_sub <- predict(insta_multiglm_bal,newdata = test_sub)
confusionMatrix(factor(insta_pred_sub),factor(insta.te$order_dow))
```

### Model: logistic regression
In order to further improve our model quality, we think about aggregating the classes of the day of week. Usually the buying behaviour is different between weekday and weekend. So we seperate the day of week into two classes weekday and weekend.
Now the outcome variable has only two categories so we can use the binomial logistic regression.

```{r}
department_WW <- department_wide %>% 
  mutate(order_ww=ifelse(order_dow %in% c(0,6) , "weekend", "weekday"))
department_WW$order_dow <- as.factor(department_WW$order_ww)
insta.WW.tr <- department_WW[insta.index,]
insta.WW.te <- department_WW[-insta.index,]
```


```{r}
train.SS <- insta.WW.tr[4:25]
insta_glm <- caret::train(order_ww~.,data = train.SS,
                                   method="glm",
                                   trControl=trainControl(method = "cv",
                                                          number = 10,
                                                          summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = T,
                                                          sampling = "down"))
```
According to the confusion matrix the balanced accuracy and kappa are higher.
```{r}
test_SS <- insta.WW.te[4:25]
insta.glm_pred <- predict(insta_glm,newdata = test_SS)
confusionMatrix(factor(insta.glm_pred),factor(insta.WW.te$order_ww))
```

