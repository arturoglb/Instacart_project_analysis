
## EDA

```{r, echo = FALSE, message = FALSE, warning=FALSE}
source(here::here("scripts/setup.R"))
```

### **Data Description**

**`orders`** (3.4m rows, 206k users):\
\* `order_id`: order identifier\
\* `user_id`: customer identifier\
\* `eval_set`: which evaluation set this order belongs in (see `SET`
described below)\
\* `order_number`: the order sequence number for this user (1 = first, n
= nth)\
\* `order_dow`: the day of the week the order was placed on\
\* `order_hour_of_day`: the hour of the day the order was placed on\
\* `days_since_prior`: days since the last order, capped at 30 (with NAs
for `order_number` = 1)

**`products`** (50k rows):\
\* `product_id`: product identifier\
\* `product_name`: name of the product\
\* `aisle_id`: foreign key\
\* `department_id`: foreign key

**`aisles`** (134 rows):\
\* `aisle_id`: aisle identifier\
\* `aisle`: the name of the aisle

**`departments`** (21 rows):\
\* `department_id`: department identifier\
\* `department`: the name of the department

**`order_products__SET`** (30m+ rows):\
\* `order_id`: foreign key\
\* `product_id`: foreign key\
\* `add_to_cart_order`: order in which each product was added to cart\
\* `reordered`: 1 if this product has been ordered by this user in the
past, 0 otherwise

where **`SET`** is one of the four following evaluation sets (`eval_set`
in `orders`):\
\* `"prior"`: orders prior to that users most recent order (\~3.2m
orders)\
\* `"train"`: training data supplied to participants (\~131k orders)\
\* `"test"`: test data reserved for machine learning competitions (\~75k
orders)\
<br>

#### **Table 1 - aisles**

```{r echo=FALSE, message=FALSE}
aisles <- read.csv(here::here("data/aisles.csv"), header = TRUE)

#missing value
sum(is.na(aisles)) #0

kable(aisles[,], caption = "The aisles table") %>%
  kable_styling(bootstrap_options = "bordered") %>%
  kableExtra::scroll_box(width = "100%", height = "250px")
```

#### **Table 2 - departments**

```{r echo=FALSE, message=FALSE}
departments <- read.csv(here::here("data/departments.csv"), header = TRUE)

#missing value
sum(is.na(departments)) #0

kable(departments[,], caption = "The departments table") %>%
  kable_styling(bootstrap_options = "bordered") %>%
  kableExtra::scroll_box(width = "100%", height = "250px")
```

#### **Table 3 - products**

```{r echo=FALSE, message=FALSE}
products <- read.csv(here::here("data/products.csv"), header = TRUE)

#missing value
sum(is.na(products)) #0

kable(products[1:50,], caption = "The products table") %>%
  kable_styling(bootstrap_options = "bordered") %>%
  kableExtra::scroll_box(width = "100%", height = "250px")
```

#### **Table 4 - order_products_train**

```{r echo=FALSE, message=FALSE}
order_products_train <- read.csv(here::here("data/order_products__train.csv"), header = TRUE)

#missing value
sum(is.na(order_products_train)) #0

kable(order_products_train[1:50,], caption = "The order_products_train table") %>%
  kable_styling(bootstrap_options = "bordered") %>%
  kableExtra::scroll_box(width = "100%", height = "250px")
```


#### **Table 6 - order**

```{r, fig.height=6, fig.width=10}
orders <- read.csv(here::here("data/orders.csv"), header = TRUE)

#missing value
sum(is.na(orders)) #206209
max(orders$user_id) #206209
#meaning that data is complete

orders %>% filter(eval_set == "train" ) %>%
  select(-order_id, -user_id, -order_number) %>%
  inspect_num() %>%
  show_plot(col_palette = 3)
```

We can observe on the first chart `days_since_prior_order` that most of
the users have a higher probability to do another purchase order after a
week from the previous purchase. Also, we can visualize on the graph
`oder_dow` that the most frequent days of ordering are Sunday's and
Monday's comparing to the rest of the week, and on the last chart
`order_hour_of_day`,we note a high demand of orders between 9am to 6pm.

#### **Table 7 - user_purchases**

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Identify the products purchased with their departments and the time and day of consumption 
user_purchases <- orders %>%
  filter(eval_set == "train") %>%
  left_join(order_products_train, by = "order_id") %>%
  left_join(products, by = "product_id") %>%
  left_join(aisles, by = c("aisle_id")) %>%
  left_join(departments, by = c("department_id")) %>%
  select(order_id, order_dow, order_hour_of_day, product_id, aisle_id,
         department)

orders <- orders %>%
  filter(eval_set == "train") %>%
  select(order_id, order_dow, order_hour_of_day)

department_wide <- user_purchases %>%
  group_by(order_id, department) %>%
  summarise(product_num = n()) %>%
  pivot_wider(names_from = "department", values_from = "product_num") %>%
  left_join(orders, by = "order_id") %>%
  relocate(order_dow, .before = `canned goods`) %>%
  relocate(order_hour_of_day, .before = `canned goods`)

department_wide[is.na(department_wide)] <- 0

#if(.Platform$OS.type == "windows") withAutoprint({
#memory.size()
#memory.size(TRUE)
#memory.limit()
#})
#memory.limit(size=56000)

#user_purchases_prtr <- rbind(user_purchases_pr, user_purchases_tr)
```
<br>
Overall, approximately 60% of the total orders are reordered products.

```{r echo=FALSE, message=FALSE}
total_percentage_reorder <-
  (sum(num_reorder_product$total_reorder)/sum(num_reorder_product$total_order))*100 
#~60%  
```
####Mandy comment -> Still keeping top 10 reordered products, top 10 reordered product by aisle. Just to see the overall.

### **Plotting**

**Top 10 reordered products**

As the result from the table, we can see that fresh fruits and packaged
vegetables fruits under produce department are the most reordered
products.

```{r echo=FALSE, message=FALSE}
num_reorder_product <- user_purchases %>% 
  group_by(product_id, product_name, aisle, department) %>%
  summarise(total_reorder = sum(reordered, na.rm = TRUE), 
            total_order = length(product_id)) %>%
  arrange(desc(total_reorder)) %>%
  mutate(percentage_reorder = (total_reorder/total_order)*100)

kable(num_reorder_product[1:10,], caption = "The top 10 reordered products") %>%
  kable_styling(bootstrap_options = "bordered") %>%
  kableExtra::scroll_box(width = "100%", height = "250px")
```

```{r, fig.height=8, fig.width=10}

fig <- num_reorder_product[1:10,] %>% plot_ly()
fig <- fig %>% add_trace(x = ~reorder(product_name, -total_reorder), 
                         y = ~total_reorder, type = 'bar', 
                         name = "The number of reorder", 
                         textposition = 'auto',
                         offsetgroup = 1,
                         marker = list(color = 'rgb(49,130,189)'))

ay <- list(
  font = list(color = "blue"),
  overlaying = "y",
  side = "right",
  title = "percentage of reorder",
  rangemode = "tozero")

fig <- fig %>% add_trace(x = ~reorder(product_name, -total_reorder), 
                         y = ~percentage_reorder, yaxis = "y2", type = 'bar', 
                         name = "Percentage of reordered products", 
                         textposition = 'auto', 
                         offsetgroup = 2,
                         marker = list(color = 'rgb(204,204,204)'))

fig <- fig %>% layout(title = "The top 10 number of reordered products", yaxis2 = ay, 
                      xaxis = list(title = "", tickangle = -45),
                      yaxis = list(title = "number of reorder"),
         margin = list(b = 100),
         barmode = 'group')

fig

```

**Top 10 reordered products by aisle**

```{r echo=FALSE, message=FALSE}
num_aisle_product <- user_purchases %>% 
  group_by(aisle, department) %>%
  summarise(total_reorder = sum(reordered, na.rm = TRUE), 
            total_order = length(product_id)) %>%
  arrange(desc(total_reorder)) %>%
  mutate(percentage_reorder = (total_reorder/total_order)*100)

kable(num_aisle_product[1:10,], caption = "The top 10 reordered products by aisle") %>%
  kable_styling(bootstrap_options = "bordered") %>%
  kableExtra::scroll_box(width = "100%", height = "250px")
```

```{r, fig.height=8, fig.width=10}

fig <- num_aisle_product[1:10,] %>% plot_ly()
fig <- fig %>% add_trace(x = ~reorder(aisle, -total_reorder), 
                         y = ~total_reorder, type = 'bar', 
                         name = "The number of reorder", 
                         textposition = 'auto',
                         offsetgroup = 1,
                         marker = list(color = 'rgb(49,130,189)'))

ay <- list(
  font = list(color = "blue"),
  overlaying = "y",
  side = "right",
  title = "percentage of reorder",
  rangemode = "tozero")

fig <- fig %>% add_trace(x = ~reorder(aisle, -total_reorder), 
                         y = ~percentage_reorder, yaxis = "y2", type = 'bar', 
                         name = "Percentage of reordered products", 
                         textposition = 'auto', 
                         offsetgroup = 2,
                         marker = list(color = 'rgb(204,204,204)'))

fig <- fig %>% layout(title = "The top 10 number of reordered products by aisle", 
                      yaxis2 = ay, 
                      xaxis = list(title = "", tickangle = -45),
                      yaxis = list(title = "number of reorder"),
         margin = list(b = 100),
         barmode = 'group')

fig

```

**Reordered products by department** 
########Mandy
Note: assuming that one row is equal to one quantity of each product
```{r echo=FALSE, message=FALSE}
num_department_product <- user_purchases %>% 
  group_by(department) %>%
  summarise(total_reorder = sum(reordered, na.rm = TRUE), 
            total_order = length(product_id)) %>%
  arrange(desc(total_reorder)) %>%
  mutate(percentage_reorder = (total_reorder/total_order)*100)

kable(num_department_product[1:10,], caption = "The top 10 reordered products by department") %>%
  kable_styling(bootstrap_options = "bordered") %>%
  kableExtra::scroll_box(width = "100%", height = "250px")
```

```{r, fig.height=8, fig.width=10}

fig <- num_department_product[,] %>% plot_ly()
fig <- fig %>% add_trace(x = ~reorder(department, -total_reorder), 
                         y = ~total_reorder, type = 'bar', 
                         name = "The number of reorder", 
                         textposition = 'auto',
                         offsetgroup = 1,
                         marker = list(color = 'rgb(49,130,189)'))

ay <- list(
  font = list(color = "blue"),
  overlaying = "y",
  side = "right",
  title = "percentage of reorder",
  rangemode = "tozero")

fig <- fig %>% add_trace(x = ~reorder(department, -total_reorder), 
                         y = ~percentage_reorder, yaxis = "y2", type = 'bar', 
                         name = "Percentage of reordered products", 
                         textposition = 'auto', 
                         offsetgroup = 2,
                         marker = list(color = 'rgb(204,204,204)'))

fig <- fig %>% layout(title = "The top 10 number of reordered products by department", 
                      yaxis2 = ay, 
                      xaxis = list(title = "", tickangle = -45),
                      yaxis = list(title = "number of reorder"),
         margin = list(b = 100),
         barmode = 'group')

fig

```

**Sales Patterns**
<br> Here, we would like to observe the pattern of sales in depth by
spilting into departments. Frist, it is the pattern of weekly sales.\
<br>

```{r, fig.height=8, fig.width=12}
sales_weekly <- user_purchases %>%
  group_by(order_dow, department)%>%
  summarise(count_byday=n())

plot_sales_weekly <- sales_weekly %>%
  ggplot(aes(x=order_dow,
             y=count_byday,
             group=department,
             color=department))+
  geom_line()+
  facet_wrap(~department, nrow=6, scales = 'free_y')+
  theme(legend.position = "bottom", legend.key.size = unit(10, "pt"))+
  labs(title = "Weekly sales pattern")

plot_sales_weekly
```

<br> From these graphs, we could observe the patterns as follow:

a.  Although in the graph shown at the beginning illustrates that the
    peak of purchase usually is on Sunday and Monday, we can see alcohol
    is the exception here. The figure increases slight from Sunday and
    reach the top on Friday, then decrease sharply on Saturday.

b.  Beverage and snacks have the entirely same pattern, they most of the
    orders happens on Sunday and Monday, but the number of orders reach
    to the peak on Monday and have a decrease trend before Friday.
    Friday is the third peak during the whole week. The least number is
    on Saturday.

c.  The rest departments of orders have similar pattern. The figures
    decrease on the top from Sunday and Monday, then rebound slightly on
    Saturday.\
    <br>

```{r, fig.height=8, fig.width=12}
sales_daily <- user_purchases %>%
  group_by(order_hour_of_day, department)%>%
  summarise(count_byday=n())

sales_daily$order_hour_of_day <- as.numeric(sales_daily$order_hour_of_day)

plot_sales_daily <- sales_daily %>%
  ggplot(aes(x=order_hour_of_day,
             y=count_byday,
             group=department,
             color=department))+
  geom_line()+
  facet_wrap(~department, nrow=6, scales = 'free_y')+
  theme(legend.position = "bottom", legend.key.size = unit(10, "pt"))+
  labs(title = "Daily sales pattern")

plot_sales_daily
```

<br> In terms of the order time of a day, we could observe from the
graphs above that for every department, the highest number of the order
happen between the period of 9am and 16pm, and they have slight
fluctuation during the period. Yet, the department we could pay more
attention to is the babies' products. Although it has the similar
pattern, it fluctuate more than the others.
<br>

### **PCA**

We analyze the association between the numbers of orders/reordered products and departments. 

Note: 
1. assuming that one row is equal to one quantity of each product.
2. when I use prior + train set, it takes a lot of run execution time (~30 mins) and keep crashing so I decided to use only training set. I think this is the reason why instacart separated data into two groups. In my opinion, the training set contains 1.3 million observations which are more than enough. 

```{r echo=FALSE, message=FALSE}
#Preparation table
pca_data <- user_purchases_tr %>% 
            select(order_id, user_id, product_name, aisle, department, reordered)%>%
            mutate(ordered = 1)
```

**PCA by department**
PCA explains the similarity of variables. There are two metrics which are correlation(scaled) and covariance(not scaled). In this project, the quantity of products is our main interest therefore, covariance is the suitable metric.

Regarding the result from PCA, we observe that the first and second components explain 46.68% and 13.76% of variance of the data. Referring to the rule of thumb which selects the number of dimensions that allow to explain at least 80% of the variation, therefore comp 1 - comp 6 are selected and around 83.9% of variance of the data are explained.

Our finding:
1. Produce has the highest variation. Also, it is highly positively correlated with Dim1 and negatively correlated with Dim2
2. The other departments including the second to sixth largest variance variables(Dairy egg, Snacks, Frozen, Beverages and Pantry) are positively correlated with Dim1 and Dim2 except Household. 
3. Household is almost uncorrelated with Dim1.

```{r echo=FALSE, message=FALSE, warning=FALSE}
data_pca_department <- pca_data %>% 
                  select(order_id, department, ordered) %>%
                  group_by(order_id, department) %>%
                  summarise(ordered_num = n()) %>%
                  pivot_wider(names_from = "department", values_from = "ordered_num")

data_pca_department[is.na(data_pca_department)] <- 0 #replace NA with 0

pca_department <- PCA(data_pca_department[,-1], scale.unit = FALSE)
pca_department
pca_department$eig
pca_department$var$coord
```
Alternative PCA to make sure that the results are the same.
```{r echo=FALSE, message=FALSE, warning=FALSE}
princomp_department <- princomp(data_pca_department[,-1], cor=FALSE)
princomp_department
summary(princomp_department)
```

<br>
Summary from TA -Supervised and unsupervised can forecast reordered
products -Supervised = Tree SVM -Unsupervised = clustering PCA

Our output: 
1. To forecast how many days are there between the prior
orders and the recorded orders: Clustering, PCA 
2. Reordered products 
3. Association between products: PCA and clustering in class


## Model: Multinomial logistic regression

Predict the day of week
```{r}
department_wide$order_dow <- as.factor(department_wide$order_dow)
department_wide$order_hour_of_day <- as.factor(department_wide$order_hour_of_day)
set.seed(12345)
insta.index <- sample(x=1:nrow(department_wide), size=0.8*nrow(department_wide), replace=FALSE)
insta.tr <- department_wide[insta.index,]
insta.te <- department_wide[-insta.index,]
```


```{r}
library(nnet)
insta.tr$order_dow2 <- relevel(as.factor(insta.tr$order_dow),ref = "0")
train <- insta.tr[3:25]
insta_multiglm <- multinom(order_dow2~.,data=train) 
#summary(insta_multiglm)
```


```{r}
test <- insta.te[3:24]
insta_pred <- predict(insta_multiglm,newdata = test)
confusionMatrix(factor(insta_pred),factor(insta.te$order_dow))
```


we need to balance the data and do cross-validation (use caret package)
```{r}
table(insta.tr$order_dow2)
```

```{r}
insta_multiglm_new <- caret::train(order_dow2~.,data=train,
                                   method="multinom",
                                   trControl=trainControl(method = "cv",
                                                          number = 10,
                                                          sampling = "down"))
# the cross-validation takes more than 1 hour? 
```


balance the data by hand
```{r}
n.3 <- min(table(insta.tr$order_dow2)) ## 12627

insta.tr.0 <- filter(insta.tr, order_dow2=="0") ## the "3" cases
insta.tr.1 <- filter(insta.tr, order_dow2=="1")
insta.tr.2 <- filter(insta.tr, order_dow2=="2")
insta.tr.3 <- filter(insta.tr, order_dow2=="3")
insta.tr.4 <- filter(insta.tr, order_dow2=="4")
insta.tr.5 <- filter(insta.tr, order_dow2=="5")
insta.tr.6 <- filter(insta.tr, order_dow2=="6")

index.0 <- sample(size=n.3, x=1:nrow(insta.tr.0), replace=FALSE) 
index.1 <- sample(size=n.3, x=1:nrow(insta.tr.1), replace=FALSE)
index.2 <- sample(size=n.3, x=1:nrow(insta.tr.2), replace=FALSE)
index.4 <- sample(size=n.3, x=1:nrow(insta.tr.4), replace=FALSE)
index.5 <- sample(size=n.3, x=1:nrow(insta.tr.5), replace=FALSE)
index.6 <- sample(size=n.3, x=1:nrow(insta.tr.6), replace=FALSE)

df.tr.subs <- data.frame(rbind(insta.tr.3, insta.tr.0[index.0,],
                               insta.tr.1[index.1,],insta.tr.2[index.2,],
                               insta.tr.4[index.4,],insta.tr.5[index.5,],
                               insta.tr.6[index.6,])) 
df.tr.subs <- df.tr.subs %>% rename("canned goods"="canned.goods",
                                    "dairy eggs"="dairy.eggs",
                      "meat seafood"="meat.seafood","personal care"="personal.care",
                      "dry goods pasta"="dry.goods.pasta"
                      )

table(df.tr.subs$order_dow2)
```

```{r}
train2 <- df.tr.subs[3:25]
insta_multiglm2 <- multinom(order_dow2~.,data=train2) 
```


```{r}
test2 <- insta.te[3:24]
insta_pred2 <- predict(insta_multiglm2,newdata = test2)
confusionMatrix(factor(insta_pred2),factor(insta.te$order_dow))
```


## Model: logistic regression
We try to separate the day of week into two groups:weekday and weekend. Then we
use logistic regression model to predict.
```{r}
department_WW <- department_wide %>% 
  mutate(order_dow=ifelse(order_dow %in% c(0,6) , "weekend", "weekday"))
department_WW$order_dow <- as.factor(department_WW$order_dow)
insta.WW.tr <- department_WW[insta.index,]
insta.WW.te <- department_WW[-insta.index,]
```


```{r}
train.SS <- insta.WW.tr[2:24]
insta.glm <- glm(order_dow~.,data = train.SS,family = "binomial") 
```


```{r}
insta.glm_pred <- predict(insta.glm,newdata = test,type = "response")
glm.pred.te <- ifelse(insta.glm_pred >= 0.5, "weekend", "weekday")
confusionMatrix(factor(glm.pred.te),factor(insta.WW.te$order_dow))
```

we balance the data and do cross-validation
```{r}
insta_glm_new <- caret::train(order_dow~.,data = train.SS,
                                   method="glm",
                                   trControl=trainControl(method = "cv",
                                                          number = 10,
                                                          summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = T,
                                                          sampling = "down"))
```


```{r}
insta.glm_pred_new <- predict(insta_glm_new,newdata = test)
confusionMatrix(factor(insta.glm_pred_new),factor(insta.WW.te$order_dow))
```


## Model: linear regression(just try and the R square is so poor, not a good model)
```{r}
insta_lm_cg <- lm(insta.tr$`canned goods`~.-order_id,data=insta.tr) %>% step()
summary(insta_lm_cg)
```


```{r}
insta_lm_cg_pred <- predict(insta_lm_cg,newdata = insta.te)
plot(insta_lm_cg_pred,insta.te$`canned goods`)
abline(0,1)
```

```{r}
library(GGally)
ggcorr(department_wide[4:24],geom = "tile")
```



